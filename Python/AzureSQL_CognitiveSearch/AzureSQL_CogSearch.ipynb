{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure SQL + Azure Cognitive Search\n",
    "This sample shows how to create and use  search index on Azure Cognitive Search when your data is in Azure SQL.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "1. Install python dotenv `pip install python-dotenv`\n",
    "   1. Enter your credentials in `example.env`\n",
    "   2. If needed, install other python packages listed in `requirements.txt`\n",
    "2. You will need pyodbc + driver to connect and interact with Azure SQL from python\n",
    "   1. Please install pyodbc [instructions](https://pypi.org/project/pyodbc/)\n",
    "   2. Install Microsoft ODBC 18 driver, [instructions here](https://learn.microsoft.com/en-us/sql/connect/odbc/microsoft-odbc-driver-for-sql-server?view=sql-server-ver16)\n",
    "3. Whitelist your IP to access your SQL server  by adding your IP from the [Azure portal](https://ms.portal.azure.com/)\n",
    "   1. Search for your SQL server resource (note: there are generaly a SQL database and a SQL server. Security / Networking is in SQL Server)\n",
    "   2. Navigate to Security / Networking\n",
    "   3. Add your IP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load environment variables and keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "# specify the name of the .env file name \n",
    "env_name = \"llm.env\" # change to use your own .env file\n",
    "config = dotenv_values(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to AZURE SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Define your Azure SQL database connection details\n",
    "server = config[\"server\"] \n",
    "database = config[\"database\"] \n",
    "username = config[\"username\"] \n",
    "password = config[\"password\"] \n",
    "driver = '{ODBC Driver 18 for SQL Server}'  # Use the appropriate driver for your system\n",
    "\n",
    "# Create a connection string\n",
    "conn_str = f\"DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "\n",
    "# Establish a connection to the Azure SQL database\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data to a table in the database\n",
    "If this is the first time you are running the notebook, you need to load our sample dataset into the database first. We will create a new table \"food_review\" and load the data from the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop previous table of same name if one exists\n",
    "cursor.execute(\"DROP TABLE IF EXISTS food_review;\")\n",
    "print(\"Finished dropping table (if existed)\")\n",
    "\n",
    "# Create a table\n",
    "cursor.execute(\"CREATE TABLE food_review (Id integer, ProductId text, UserId text, ProfileName text, HelpfulnessNumerator integer, HelpfulnessDenominator integer, Score integer, Time bigint, Summary text, Text text);\")\n",
    "print(\"Finished creating table\")\n",
    "\n",
    "# Create a index\n",
    "cursor.execute(\"CREATE INDEX idx_Id ON food_review(Id);\")\n",
    "print(\"Finished creating index\")\n",
    "\n",
    "##Load Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../../DataSet/Reviews_small.csv')\n",
    "\n",
    "# Specify the batch size\n",
    "batch_size = 30\n",
    "table_name = \"food_review\" \n",
    "\n",
    "# Split the dataframe into batches\n",
    "batches = [df[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "#Iterate over each batch and insert the data into the database\n",
    "for batch in batches:\n",
    "    # Convert the batch dataframe to a list of tuples for bulk insertion\n",
    "    rows = [tuple(row) for row in batch.itertuples(index=False)]\n",
    "    \n",
    "    # Define the SQL query for bulk insertion\n",
    "    query = f\"INSERT INTO {table_name} (Id, ProductId, UserId, ProfileName, HelpfulnessNumerator, HelpfulnessDenominator, Score, Time, Summary, Text) \\\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "    cursor.executemany(query, rows)\n",
    "    \n",
    "    \n",
    "# Insert the data from the CSV file into the database table row by row\n",
    "# table_name = \"food_review\"\n",
    "# for row in df.itertuples(index=False):\n",
    "#     values = ', '.join(['?'] * len(row))\n",
    "#     insert_query = f\"INSERT INTO {table_name} VALUES ({values});\"\n",
    "#     cursor.execute(insert_query, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already established a connection and have a cursor object\n",
    "\n",
    "# Execute the SELECT statement\n",
    "try:\n",
    "    cursor.execute(\"SELECT count(Id) FROM food_review;\")\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "except (Exception, Error) as e:\n",
    "    print(f\"Error executing SELECT statement: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data from database and store the embedding in CogSearch \n",
    "In this step, we will retrieve the id and concatenated data of desired columns from database first. Then we will use azure open ai to get the text embedding. We will then store the text embedding in azure CogSearch for the future retrieval purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already established a connection and have a cursor object\n",
    "\n",
    "# Execute the SELECT statement\n",
    "try:\n",
    "    cursor.execute(\"SELECT id, CONCAT('productid: ', productid, ' ', 'score: ', score, ' ', 'text: ', text) AS concat FROM food_review;\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "except (Exception, Error) as e:\n",
    "    print(f\"Error executing SELECT statement: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the content and generate the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_type = config[\"openai_api_type\"] #\"azure\"\n",
    "openai.api_key = config['openai_api_key']\n",
    "openai.api_base = config['openai_api_base'] #\"https://synapseml-openai.openai.azure.com/\"\n",
    "openai.api_version = config['openai_api_version'] \n",
    "\n",
    "\n",
    "def createEmbeddings(text):\n",
    "    response = openai.Embedding.create(input=text , engine=config[\"openai_deployment_embedding\"])\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings\n",
    "\n",
    "content_embeddings = []\n",
    "idx = []\n",
    "for row in rows:\n",
    "    idx.append(row[0])\n",
    "    content_embeddings.append(createEmbeddings(row[1]))\n",
    "\n",
    "df = pd.DataFrame({'embeddings': content_embeddings}, index=idx) # storing embeddings in a dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the embeddings in Azure Cognitive Search Vector Store\n",
    "\n",
    "[AzureCogSearch](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) provides a simple interface to create a vector database, store and retrieve data using vector search. You can read more about [here](https://github.com/Azure/cognitive-search-vector-pr/tree/main) more about Vector Search.\n",
    "\n",
    "There are two steps to store data in AzureCogSearch vector database:\n",
    "- First, we create the index (or schema) of the vector database\n",
    "- Second, we add the chunked documents and their embeddings to the vector datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "# Azure Cognitive Search\n",
    "cogsearch_name = config[\"cogsearch_name\"] #TODO: fill in your cognitive search name\n",
    "cogsearch_index_name = config[\"cogsearch_index_name\"] #TODO: fill in your index name: must only contain lowercase, numbers, and dashes\n",
    "cogsearch_api_key = config[\"cogsearch_api_key\"] #TODO: fill in your api key with admin key\n",
    "\n",
    "EMBEDDING_LENGTH = 1536\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Index for Cog Search with fields as id, and contentVector\n",
    "# Note the datatypes for each field below\n",
    "\n",
    "url = f\"https://{cogsearch_name}.search.windows.net/indexes/{cogsearch_index_name}?api-version=2023-07-01-Preview\"\n",
    "payload = json.dumps({\n",
    "  \"name\": cogsearch_index_name,\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"name\": \"id\",\n",
    "      \"type\": \"Edm.String\",\n",
    "      \"key\": True,\n",
    "      \"filterable\": True\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"contentVector\",\n",
    "      \"type\": \"Collection(Edm.Single)\",\n",
    "      \"searchable\": True,\n",
    "      \"retrievable\": True,\n",
    "      \"dimensions\": EMBEDDING_LENGTH,\n",
    "      \"vectorSearchConfiguration\": \"vectorConfig\"\n",
    "    }\n",
    "  ],\n",
    "  \"vectorSearch\": {\n",
    "    \"algorithmConfigurations\": [\n",
    "      {\n",
    "        \"name\": \"vectorConfig\",\n",
    "        \"kind\": \"hnsw\",\n",
    "        # \"hnswParameters\": {\n",
    "        #   \"m\": 4,\n",
    "        #   \"efConstruction\": 400,\n",
    "        #   \"metric\": \"cosine\"\n",
    "        # }\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"semantic\": {\n",
    "    \"configurations\": [\n",
    "      {\n",
    "        \"name\": \"my-semantic-config\",\n",
    "        \"prioritizedFields\": {\n",
    "          \"prioritizedContentFields\": [\n",
    "            {\n",
    "              \"fieldName\": \"id\"\n",
    "            }\n",
    "          ],\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'api-key': cogsearch_api_key\n",
    "}\n",
    "\n",
    "response = requests.request(\"PUT\", url, headers=headers, data=payload)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_append_payload(df):\n",
    "    \"\"\"append payload for batch insertion (note: max 1000 rows per insertion) of embeddings to Cognitive Search\"\"\"\n",
    "    value_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        value_list.append(\n",
    "            {\n",
    "            \"id\": str(index),\n",
    "            \"contentVector\": row['embeddings'],\n",
    "            \"@search.action\": \"upload\"\n",
    "            }\n",
    "        )\n",
    "    print('payload of size {}'.format(len(value_list)))\n",
    "    print('start: {}'.format(value_list[0]))\n",
    "    print('end: {}'.format(value_list[-1]))\n",
    "    payload = json.dumps({\n",
    "        \"value\": value_list\n",
    "    })\n",
    "    return payload\n",
    "\n",
    "def BatchInsertToCogSearch(df):\n",
    "    \"\"\"Batch insertion of embedding to Cognitive Search, note: column name must be 'embeddings'\"\"\"\n",
    "    url = f\"https://{cogsearch_name}.search.windows.net/indexes/{cogsearch_index_name}/docs/index?api-version=2023-07-01-Preview\"\n",
    "    payload = batch_append_payload(df)\n",
    "    headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'api-key': cogsearch_api_key,\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    print(response.json())\n",
    "\n",
    "    if response.status_code == 200 or response.status_code == 201:\n",
    "        return \"Success\"\n",
    "    else:\n",
    "        return \"Failure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchInsertToCogSearch(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Asks a Question \n",
    "In this step, the code will convert the user's question to an embedding and then retieve the top K document chunks based on the users' question using the cosine similirity. Please note that other similarity metrics can also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userQuestion = \"Great Taffy\"\n",
    "retrieve_k = 3 # Retrieve the top 2 documents from vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve k chnuks\n",
    "def retrieve_k_chunk(k, questionEmbedding):\n",
    "    # Retrieve the top K entries\n",
    "    url = f\"https://{cogsearch_name}.search.windows.net/indexes/{cogsearch_index_name}/docs/search?api-version=2023-07-01-Preview\"\n",
    "\n",
    "    payload = json.dumps({\n",
    "    \"vector\": {\n",
    "        \"value\": questionEmbedding,\n",
    "        \"fields\": \"contentVector\",\n",
    "        \"k\": k\n",
    "    }\n",
    "    })\n",
    "    headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'api-key': cogsearch_api_key,\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    output = json.loads(response.text)\n",
    "    print(response.status_code)\n",
    "    return output\n",
    "\n",
    "# Generate embeddings for the question and retrieve the top k document chunks\n",
    "questionEmbedding = createEmbeddings(userQuestion)\n",
    "output = retrieve_k_chunk(retrieve_k, questionEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(output['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the top k ids to retrieve the actual text from the database \n",
    "top_ids = []\n",
    "for i in range(len(output['value'])):\n",
    "    top_ids.append(int(output['value'][i]['id']))\n",
    "\n",
    "print(top_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve text from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already established a connection and have a cursor object\n",
    "top_ids_string = ', '.join(map(str, top_ids))\n",
    "\n",
    "sql = f\"SELECT CONCAT('productid: ', productid, ' ', 'score: ', score, ' ', 'text: ', text) AS concat FROM food_review WHERE Id IN ({top_ids_string})\"\n",
    "\n",
    "# Execute the SELECT statement\n",
    "try:\n",
    "    cursor.execute(sql)    \n",
    "    top_rows = cursor.fetchall()\n",
    "    for row in top_rows:\n",
    "        print(row)\n",
    "except (Exception) as e:\n",
    "    print(f\"Error executing SELECT statement: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL: Offer Response to User's Question\n",
    "In order to offer a response, a user can either follow a simple prompting method as shown below or leverage more sophisticated ways used by other libraries, such as [langchain](https://python.langchain.com/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompting directly using Azure Open AI service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt template \n",
    "template = \"\"\"\n",
    "    context :{context}\n",
    "    Answer the question based on the context above. Provide the product id associated with the answer as well. If the\n",
    "    information to answer the question is not present in the given context then reply \"I don't know\".\n",
    "    Question: {query}\n",
    "    Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the context from the top_rows\n",
    "context = \"\"\n",
    "for row in top_rows:\n",
    "    context += row[0]\n",
    "    context += \"\\n\"\n",
    "    \n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(userQuestion)\n",
    "prompt = template.format(context=context, query=userQuestion)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = openai.Completion.create(\n",
    "    engine= config[\"openai_deployment_completion\"],\n",
    "    prompt=prompt,\n",
    "    max_tokens=1024,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "print(\"prompt: \", prompt)\n",
    "print('~~~~~')\n",
    "# print(\"response: \", response['choices'][0]['text'].replace('\\n', '').replace(' .', '.').strip())\n",
    "print(response['choices'][0]['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "nanogpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
